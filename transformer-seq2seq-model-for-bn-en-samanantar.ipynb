{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ef37d5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-03T07:30:10.505960Z",
     "iopub.status.busy": "2024-03-03T07:30:10.505649Z",
     "iopub.status.idle": "2024-03-03T07:30:11.333058Z",
     "shell.execute_reply": "2024-03-03T07:30:11.332181Z"
    },
    "papermill": {
     "duration": 0.839942,
     "end_time": "2024-03-03T07:30:11.335332",
     "exception": false,
     "start_time": "2024-03-03T07:30:10.495390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/samanantar/final_data/en-ta/train.ta\n",
      "/kaggle/input/samanantar/final_data/en-ta/train.en\n",
      "/kaggle/input/samanantar/final_data/en-ml/train.ml\n",
      "/kaggle/input/samanantar/final_data/en-ml/train.en\n",
      "/kaggle/input/samanantar/final_data/en-as/train.as\n",
      "/kaggle/input/samanantar/final_data/en-as/train.en\n",
      "/kaggle/input/samanantar/final_data/en-kn/train.kn\n",
      "/kaggle/input/samanantar/final_data/en-kn/train.en\n",
      "/kaggle/input/samanantar/final_data/en-pa/train.pa\n",
      "/kaggle/input/samanantar/final_data/en-pa/train.en\n",
      "/kaggle/input/samanantar/final_data/en-mr/train.mr\n",
      "/kaggle/input/samanantar/final_data/en-mr/train.en\n",
      "/kaggle/input/samanantar/final_data/en-hi/train.hi\n",
      "/kaggle/input/samanantar/final_data/en-hi/train.en\n",
      "/kaggle/input/samanantar/final_data/en-bn/train.bn\n",
      "/kaggle/input/samanantar/final_data/en-bn/train.en\n",
      "/kaggle/input/samanantar/final_data/en-te/train.te\n",
      "/kaggle/input/samanantar/final_data/en-te/train.en\n",
      "/kaggle/input/samanantar/final_data/en-or/train.or\n",
      "/kaggle/input/samanantar/final_data/en-or/train.en\n",
      "/kaggle/input/samanantar/final_data/en-gu/train.gu\n",
      "/kaggle/input/samanantar/final_data/en-gu/train.en\n",
      "/kaggle/input/transformer-seq2seq-model-for-bn-en-samanantar/__results__.html\n",
      "/kaggle/input/transformer-seq2seq-model-for-bn-en-samanantar/filename.pkl\n",
      "/kaggle/input/transformer-seq2seq-model-for-bn-en-samanantar/__notebook__.ipynb\n",
      "/kaggle/input/transformer-seq2seq-model-for-bn-en-samanantar/__output__.json\n",
      "/kaggle/input/transformer-seq2seq-model-for-bn-en-samanantar/custom.css\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5240472d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:30:11.354263Z",
     "iopub.status.busy": "2024-03-03T07:30:11.353456Z",
     "iopub.status.idle": "2024-03-03T07:30:16.377036Z",
     "shell.execute_reply": "2024-03-03T07:30:16.376263Z"
    },
    "papermill": {
     "duration": 5.035149,
     "end_time": "2024-03-03T07:30:16.379311",
     "exception": false,
     "start_time": "2024-03-03T07:30:11.344162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "from torch.utils.data import Dataset\n",
    "# import gc\n",
    "SRC_LANGUAGE = 'en'\n",
    "TGT_LANGUAGE = 'bn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcdda39",
   "metadata": {
    "papermill": {
     "duration": 0.008305,
     "end_time": "2024-03-03T07:30:16.396352",
     "exception": false,
     "start_time": "2024-03-03T07:30:16.388047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac2d8d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:30:16.414650Z",
     "iopub.status.busy": "2024-03-03T07:30:16.413948Z",
     "iopub.status.idle": "2024-03-03T07:30:29.653493Z",
     "shell.execute_reply": "2024-03-03T07:30:29.652345Z"
    },
    "papermill": {
     "duration": 13.251144,
     "end_time": "2024-03-03T07:30:29.655929",
     "exception": false,
     "start_time": "2024-03-03T07:30:16.404785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6787ab87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:30:29.675706Z",
     "iopub.status.busy": "2024-03-03T07:30:29.675370Z",
     "iopub.status.idle": "2024-03-03T07:30:45.412190Z",
     "shell.execute_reply": "2024-03-03T07:30:45.410611Z"
    },
    "papermill": {
     "duration": 15.749119,
     "end_time": "2024-03-03T07:30:45.414425",
     "exception": false,
     "start_time": "2024-03-03T07:30:29.665306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bnlp_toolkit\r\n",
      "  Downloading bnlp_toolkit-4.0.0-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from bnlp_toolkit) (0.2.0)\r\n",
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (from bnlp_toolkit) (4.3.2)\r\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from bnlp_toolkit) (3.2.4)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bnlp_toolkit) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bnlp_toolkit) (1.11.4)\r\n",
      "Collecting sklearn-crfsuite (from bnlp_toolkit)\r\n",
      "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from bnlp_toolkit) (4.66.1)\r\n",
      "Collecting ftfy (from bnlp_toolkit)\r\n",
      "  Downloading ftfy-6.1.3-py3-none-any.whl.metadata (6.2 kB)\r\n",
      "Collecting emoji==1.7.0 (from bnlp_toolkit)\r\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bnlp_toolkit) (2.31.0)\r\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->bnlp_toolkit) (0.2.13)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim->bnlp_toolkit) (6.4.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->bnlp_toolkit) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bnlp_toolkit) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bnlp_toolkit) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bnlp_toolkit) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bnlp_toolkit) (2024.2.2)\r\n",
      "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite->bnlp_toolkit)\r\n",
      "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\r\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from sklearn-crfsuite->bnlp_toolkit) (0.9.0)\r\n",
      "Downloading bnlp_toolkit-4.0.0-py3-none-any.whl (22 kB)\r\n",
      "Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\r\n",
      "Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\r\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=4727c60439a0d053a1153e4af92ffc4e46b7eda4a946a0e9fcfe562c717e1cb0\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\r\n",
      "Successfully built emoji\r\n",
      "Installing collected packages: python-crfsuite, emoji, sklearn-crfsuite, ftfy, bnlp_toolkit\r\n",
      "  Attempting uninstall: emoji\r\n",
      "    Found existing installation: emoji 2.10.1\r\n",
      "    Uninstalling emoji-2.10.1:\r\n",
      "      Successfully uninstalled emoji-2.10.1\r\n",
      "Successfully installed bnlp_toolkit-4.0.0 emoji-1.7.0 ftfy-6.1.3 python-crfsuite-0.9.10 sklearn-crfsuite-0.3.6\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bnlp_toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bd2dfb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:30:45.438123Z",
     "iopub.status.busy": "2024-03-03T07:30:45.437393Z",
     "iopub.status.idle": "2024-03-03T07:31:02.646231Z",
     "shell.execute_reply": "2024-03-03T07:31:02.645445Z"
    },
    "papermill": {
     "duration": 17.223064,
     "end_time": "2024-03-03T07:31:02.648589",
     "exception": false,
     "start_time": "2024-03-03T07:30:45.425525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bnlp import NLTKTokenizer\n",
    "\n",
    "bnltk = NLTKTokenizer()\n",
    "text = \"আমি ভাত খাই। সে বাজারে যায়। তিনি কি সত্যিই ভালো মানুষ?\"\n",
    "word_tokens = bnltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae949e7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:31:02.672681Z",
     "iopub.status.busy": "2024-03-03T07:31:02.672096Z",
     "iopub.status.idle": "2024-03-03T07:31:02.676823Z",
     "shell.execute_reply": "2024-03-03T07:31:02.675886Z"
    },
    "papermill": {
     "duration": 0.018535,
     "end_time": "2024-03-03T07:31:02.679081",
     "exception": false,
     "start_time": "2024-03-03T07:31:02.660546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['আমি', 'ভাত', 'খাই', '।', 'সে', 'বাজারে', 'যায়', '।', 'তিনি', 'কি', 'সত্যিই', 'ভালো', 'মানুষ', '?']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d9788dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:31:02.701404Z",
     "iopub.status.busy": "2024-03-03T07:31:02.701087Z",
     "iopub.status.idle": "2024-03-03T07:31:02.704716Z",
     "shell.execute_reply": "2024-03-03T07:31:02.703863Z"
    },
    "papermill": {
     "duration": 0.017021,
     "end_time": "2024-03-03T07:31:02.706669",
     "exception": false,
     "start_time": "2024-03-03T07:31:02.689648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def tokenize(text):\n",
    "#     return bnltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beff3567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:31:02.729060Z",
     "iopub.status.busy": "2024-03-03T07:31:02.728781Z",
     "iopub.status.idle": "2024-03-03T07:31:06.195283Z",
     "shell.execute_reply": "2024-03-03T07:31:06.194324Z"
    },
    "papermill": {
     "duration": 3.480148,
     "end_time": "2024-03-03T07:31:06.197429",
     "exception": false,
     "start_time": "2024-03-03T07:31:02.717281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c13841e70f41dea33ff86b57b554fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/491 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e666b4c2654f95a45f766bb3d508fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/2.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['আমি', 'ভাল', '##ে', '##া', 'বাংলা', '##য', 'গান', 'গাই', '।']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "bnbert_tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "text = \"আমি ভালো বাংলায় গান গাই।\"\n",
    "bnbert_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15c28dfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:31:06.221243Z",
     "iopub.status.busy": "2024-03-03T07:31:06.220734Z",
     "iopub.status.idle": "2024-03-03T07:31:06.225123Z",
     "shell.execute_reply": "2024-03-03T07:31:06.224302Z"
    },
    "papermill": {
     "duration": 0.018169,
     "end_time": "2024-03-03T07:31:06.226982",
     "exception": false,
     "start_time": "2024-03-03T07:31:06.208813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return bnbert_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41433cd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:31:06.250251Z",
     "iopub.status.busy": "2024-03-03T07:31:06.249998Z",
     "iopub.status.idle": "2024-03-03T07:31:06.253874Z",
     "shell.execute_reply": "2024-03-03T07:31:06.253163Z"
    },
    "papermill": {
     "duration": 0.017883,
     "end_time": "2024-03-03T07:31:06.255759",
     "exception": false,
     "start_time": "2024-03-03T07:31:06.237876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbed2656",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:31:06.279043Z",
     "iopub.status.busy": "2024-03-03T07:31:06.278780Z",
     "iopub.status.idle": "2024-03-03T07:31:06.286567Z",
     "shell.execute_reply": "2024-03-03T07:31:06.285732Z"
    },
    "papermill": {
     "duration": 0.021569,
     "end_time": "2024-03-03T07:31:06.288482",
     "exception": false,
     "start_time": "2024-03-03T07:31:06.266913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_src='../input/samanantar/final_data/en-bn/train.en'\n",
    "dataset_tgt='../input/samanantar/final_data/en-bn/train.bn'\n",
    "class CustomizedSamanantar(Dataset):\n",
    "    def __init__(self, dataset_dir,tgt_dir,split=\"train\"):\n",
    "        with open(dataset_dir,\"r\") as f:\n",
    "            self.dataset=f.readlines()\n",
    "        with open(tgt_dir,\"r\") as f:\n",
    "            self.labels=f.readlines()\n",
    "        if split==\"train\":\n",
    "            self.dataset=self.dataset[:35000]\n",
    "            self.labels=self.labels[:35000]\n",
    "        if split==\"valid\":\n",
    "            self.dataset=self.dataset[30000:31000]\n",
    "            self.labels=self.labels[30000:31000]\n",
    "        if split==\"test\":\n",
    "            self.dataset=self.dataset[50000:]\n",
    "            self.labels=self.labels[50000:]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.dataset[idx],self.labels[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bcff11a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:31:06.311733Z",
     "iopub.status.busy": "2024-03-03T07:31:06.311472Z",
     "iopub.status.idle": "2024-03-03T07:31:25.889811Z",
     "shell.execute_reply": "2024-03-03T07:31:25.888588Z"
    },
    "papermill": {
     "duration": 19.592698,
     "end_time": "2024-03-03T07:31:25.892325",
     "exception": false,
     "start_time": "2024-03-03T07:31:06.299627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.10)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\r\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20a081fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:31:25.918586Z",
     "iopub.status.busy": "2024-03-03T07:31:25.918230Z",
     "iopub.status.idle": "2024-03-03T07:32:04.806595Z",
     "shell.execute_reply": "2024-03-03T07:32:04.805791Z"
    },
    "papermill": {
     "duration": 38.90403,
     "end_time": "2024-03-03T07:32:04.808984",
     "exception": false,
     "start_time": "2024-03-03T07:31:25.904954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_iter = CustomizedSamanantar(dataset_src,dataset_tgt,split=\"train\")\n",
    "val_iter = CustomizedSamanantar(dataset_src,dataset_tgt,split=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e49a04ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:32:04.835773Z",
     "iopub.status.busy": "2024-03-03T07:32:04.835494Z",
     "iopub.status.idle": "2024-03-03T07:32:17.632193Z",
     "shell.execute_reply": "2024-03-03T07:32:17.631299Z"
    },
    "papermill": {
     "duration": 12.812466,
     "end_time": "2024-03-03T07:32:17.634692",
     "exception": false,
     "start_time": "2024-03-03T07:32:04.822226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_transform={}\n",
    "vocab_transform = {}\n",
    "\n",
    "token_transform[TGT_LANGUAGE]=tokenize\n",
    "token_transform[SRC_LANGUAGE]=get_tokenizer('spacy',language='en_core_web_sm')\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "# train_iter=CustomizedSamanantar(dataset_src,dataset_tgt)\n",
    "for ln in [SRC_LANGUAGE,TGT_LANGUAGE]:\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                        min_freq=1,\n",
    "                                                        specials=special_symbols,\n",
    "                                                        special_first=True)\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07164a33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:32:17.661143Z",
     "iopub.status.busy": "2024-03-03T07:32:17.660535Z",
     "iopub.status.idle": "2024-03-03T07:32:17.678860Z",
     "shell.execute_reply": "2024-03-03T07:32:17.678036Z"
    },
    "papermill": {
     "duration": 0.033374,
     "end_time": "2024-03-03T07:32:17.680781",
     "exception": false,
     "start_time": "2024-03-03T07:32:17.647407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network \n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, \n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e6e98a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:32:17.706228Z",
     "iopub.status.busy": "2024-03-03T07:32:17.705971Z",
     "iopub.status.idle": "2024-03-03T07:32:17.712804Z",
     "shell.execute_reply": "2024-03-03T07:32:17.711989Z"
    },
    "papermill": {
     "duration": 0.021738,
     "end_time": "2024-03-03T07:32:17.714730",
     "exception": false,
     "start_time": "2024-03-03T07:32:17.692992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb8b6aa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:32:17.740413Z",
     "iopub.status.busy": "2024-03-03T07:32:17.740115Z",
     "iopub.status.idle": "2024-03-03T07:32:19.478178Z",
     "shell.execute_reply": "2024-03-03T07:32:19.477420Z"
    },
    "papermill": {
     "duration": 1.753744,
     "end_time": "2024-03-03T07:32:19.480691",
     "exception": false,
     "start_time": "2024-03-03T07:32:17.726947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 20\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, \n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4df0a168",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:32:19.507203Z",
     "iopub.status.busy": "2024-03-03T07:32:19.506608Z",
     "iopub.status.idle": "2024-03-03T07:32:19.511438Z",
     "shell.execute_reply": "2024-03-03T07:32:19.510512Z"
    },
    "papermill": {
     "duration": 0.02019,
     "end_time": "2024-03-03T07:32:19.513433",
     "exception": false,
     "start_time": "2024-03-03T07:32:19.493243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36307 15112\n"
     ]
    }
   ],
   "source": [
    "print(SRC_VOCAB_SIZE,TGT_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0860952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:32:19.539254Z",
     "iopub.status.busy": "2024-03-03T07:32:19.538962Z",
     "iopub.status.idle": "2024-03-03T07:32:19.547922Z",
     "shell.execute_reply": "2024-03-03T07:32:19.547078Z"
    },
    "papermill": {
     "duration": 0.024095,
     "end_time": "2024-03-03T07:32:19.550018",
     "exception": false,
     "start_time": "2024-03-03T07:32:19.525923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "633fe4b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:32:19.575880Z",
     "iopub.status.busy": "2024-03-03T07:32:19.575629Z",
     "iopub.status.idle": "2024-03-03T07:32:19.586293Z",
     "shell.execute_reply": "2024-03-03T07:32:19.585465Z"
    },
    "papermill": {
     "duration": 0.026116,
     "end_time": "2024-03-03T07:32:19.588371",
     "exception": false,
     "start_time": "2024-03-03T07:32:19.562255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    \n",
    "    for src, tgt in train_dataloader:\n",
    "#         print(len(src),len(tgt),src,tgt)\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        \n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20a4e3a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:32:19.613919Z",
     "iopub.status.busy": "2024-03-03T07:32:19.613639Z",
     "iopub.status.idle": "2024-03-03T07:32:28.203406Z",
     "shell.execute_reply": "2024-03-03T07:32:28.202322Z"
    },
    "papermill": {
     "duration": 8.604957,
     "end_time": "2024-03-03T07:32:28.205556",
     "exception": false,
     "start_time": "2024-03-03T07:32:19.600599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000\n"
     ]
    }
   ],
   "source": [
    "dataset=CustomizedSamanantar(dataset_src,dataset_tgt)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37d5d127",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:32:28.232885Z",
     "iopub.status.busy": "2024-03-03T07:32:28.232566Z",
     "iopub.status.idle": "2024-03-03T07:32:28.237292Z",
     "shell.execute_reply": "2024-03-03T07:32:28.236347Z"
    },
    "papermill": {
     "duration": 0.021236,
     "end_time": "2024-03-03T07:32:28.239364",
     "exception": false,
     "start_time": "2024-03-03T07:32:28.218128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('However, he did not give further details.\\n', 'তবে এর চেয়ে বেশি তথ্য তিনি দেননি।\\n')\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "339f9283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:32:28.265349Z",
     "iopub.status.busy": "2024-03-03T07:32:28.265050Z",
     "iopub.status.idle": "2024-03-03T07:32:28.275909Z",
     "shell.execute_reply": "2024-03-03T07:32:28.275050Z"
    },
    "papermill": {
     "duration": 0.025849,
     "end_time": "2024-03-03T07:32:28.277794",
     "exception": false,
     "start_time": "2024-03-03T07:32:28.251945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to generate output sequence using greedy algorithm \n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ca2caee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:32:28.307683Z",
     "iopub.status.busy": "2024-03-03T07:32:28.307373Z",
     "iopub.status.idle": "2024-03-03T07:32:28.311509Z",
     "shell.execute_reply": "2024-03-03T07:32:28.310531Z"
    },
    "papermill": {
     "duration": 0.021744,
     "end_time": "2024-03-03T07:32:28.313700",
     "exception": false,
     "start_time": "2024-03-03T07:32:28.291956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_iter = CustomizedSamanantar(dataset_src,dataset_tgt,split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90fa00ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:32:28.341707Z",
     "iopub.status.busy": "2024-03-03T07:32:28.341050Z",
     "iopub.status.idle": "2024-03-03T07:32:30.009819Z",
     "shell.execute_reply": "2024-03-03T07:32:30.008740Z"
    },
    "papermill": {
     "duration": 1.68421,
     "end_time": "2024-03-03T07:32:30.012276",
     "exception": false,
     "start_time": "2024-03-03T07:32:28.328066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "try:\n",
    "    with open('../input/transformer-seq2seq-model-for-bn-en-samanantar/filename.pkl', 'rb') as f:\n",
    "        transformer = pickle.load(f)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf97bcb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:32:30.039020Z",
     "iopub.status.busy": "2024-03-03T07:32:30.038703Z",
     "iopub.status.idle": "2024-03-03T07:32:30.309043Z",
     "shell.execute_reply": "2024-03-03T07:32:30.308189Z"
    },
    "papermill": {
     "duration": 0.286204,
     "end_time": "2024-03-03T07:32:30.311578",
     "exception": false,
     "start_time": "2024-03-03T07:32:30.025374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../input/transformer-seq2seq-model-for-bn-en-samanantar/filename.pkl', 'rb') as f:\n",
    "        transformer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cebd4796",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T07:32:30.338716Z",
     "iopub.status.busy": "2024-03-03T07:32:30.338416Z",
     "iopub.status.idle": "2024-03-03T13:27:05.202734Z",
     "shell.execute_reply": "2024-03-03T13:27:05.201755Z"
    },
    "papermill": {
     "duration": 21274.912417,
     "end_time": "2024-03-03T13:27:05.236862",
     "exception": false,
     "start_time": "2024-03-03T07:32:30.324445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  1 Duration =  84.96427957200001\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  2 Duration =  84.27337560399997\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  3 Duration =  84.33012953500003\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  4 Duration =  84.29385885999989\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  5 Duration =  84.25829160900003\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  6 Duration =  84.26578017600002\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  7 Duration =  84.30618490699999\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  8 Duration =  84.38695165299998\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  9 Duration =  84.26431228899992\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  10 Duration =  84.22014156699993\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  11 Duration =  84.26701843000001\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  12 Duration =  84.18773965200012\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  13 Duration =  84.22398373800002\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  14 Duration =  84.25646056599999\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  15 Duration =  84.2204862420001\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  16 Duration =  84.26826329200003\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  17 Duration =  84.31842492200008\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  18 Duration =  84.21755031999987\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  19 Duration =  84.13060920900011\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  20 Duration =  84.27087168399999\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  21 Duration =  84.21629464699981\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  22 Duration =  84.32906994599989\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  23 Duration =  84.24135467399992\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  24 Duration =  84.17987382800038\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  25 Duration =  84.21839775699982\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  26 Duration =  84.11533259899988\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  27 Duration =  84.22174675799988\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  28 Duration =  84.22512214700009\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  29 Duration =  84.22975644899998\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  30 Duration =  84.15733450000016\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  31 Duration =  84.11723383900016\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  32 Duration =  84.18475075900005\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  33 Duration =  84.18847807099974\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  34 Duration =  84.1158824019999\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  35 Duration =  83.95957946299995\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  36 Duration =  84.37879557499991\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  37 Duration =  84.43417863800005\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  38 Duration =  84.0861341990003\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  39 Duration =  84.03277513900002\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  40 Duration =  84.24273432899963\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  41 Duration =  84.16849633499987\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  42 Duration =  84.0842311739998\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  43 Duration =  84.0087602059998\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  44 Duration =  84.05948252600001\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  45 Duration =  84.08189799900038\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  46 Duration =  83.9753822699995\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  47 Duration =  84.16103178100002\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  48 Duration =  84.09397604300011\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  49 Duration =  84.03753046099973\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  50 Duration =  84.1540818100002\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  51 Duration =  84.17000026000005\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  52 Duration =  84.13166864899995\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  53 Duration =  84.04488753299938\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  54 Duration =  84.1474822830005\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  55 Duration =  84.17632050700013\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  56 Duration =  84.11800583599961\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  57 Duration =  84.17036102200018\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  58 Duration =  84.10607866300052\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  59 Duration =  84.04937262199928\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  60 Duration =  84.05235880599957\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  61 Duration =  84.10980003700024\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  62 Duration =  84.01398518099995\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  63 Duration =  84.06548221899993\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  64 Duration =  84.08613271599916\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  65 Duration =  84.101230235\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  66 Duration =  84.02128878199983\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  67 Duration =  84.04568153700075\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  68 Duration =  84.0226170650003\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  69 Duration =  84.08272199100065\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  70 Duration =  84.06620786599979\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  71 Duration =  84.09112176999952\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  72 Duration =  84.05208568000035\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  73 Duration =  84.78200537700013\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  74 Duration =  84.59296583099967\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  75 Duration =  84.04741338800068\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  76 Duration =  84.05415874499977\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  77 Duration =  84.07493418600006\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  78 Duration =  84.05929364900021\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  79 Duration =  84.06826490799995\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  80 Duration =  84.08843631199943\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  81 Duration =  84.02125067499946\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  82 Duration =  84.23107457900005\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  83 Duration =  84.23357801400016\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  84 Duration =  84.17842981800004\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  85 Duration =  84.13018614500015\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  86 Duration =  84.28880825100077\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  87 Duration =  84.41693055500036\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  88 Duration =  84.48891169200033\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  89 Duration =  84.5180934580003\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  90 Duration =  84.50815749499998\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  91 Duration =  84.78669067299961\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  92 Duration =  84.83779582499938\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  93 Duration =  84.02651820499977\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  94 Duration =  84.15170574699914\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  95 Duration =  84.35516434700003\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  96 Duration =  83.98858107200067\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  97 Duration =  84.07088583500081\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  98 Duration =  84.02415113400093\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  99 Duration =  83.99510936000115\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  100 Duration =  84.00997083400034\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  101 Duration =  84.0700981559985\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  102 Duration =  84.0933311789995\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  103 Duration =  84.10142976499992\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  104 Duration =  84.05005487600101\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  105 Duration =  84.05087482899944\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  106 Duration =  84.00738327600084\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  107 Duration =  84.01448128799893\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  108 Duration =  84.15965609399973\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  109 Duration =  84.19962251499965\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  110 Duration =  84.01259168099932\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  111 Duration =  83.99996983299934\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  112 Duration =  84.00029663299938\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  113 Duration =  84.02269378599885\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  114 Duration =  84.05913875499937\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  115 Duration =  84.06086339000103\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  116 Duration =  84.16862948800008\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  117 Duration =  84.10393216000011\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  118 Duration =  83.9832263190001\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  119 Duration =  84.11285140400105\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  120 Duration =  84.03608116400028\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  121 Duration =  84.06867949799926\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  122 Duration =  83.9725340570003\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  123 Duration =  83.90398068000104\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  124 Duration =  83.69796821399905\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  125 Duration =  83.78637905099822\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  126 Duration =  83.7663022680008\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  127 Duration =  83.86654092299977\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  128 Duration =  83.76877679999961\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  129 Duration =  83.78112973699899\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  130 Duration =  83.76450813700103\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  131 Duration =  83.81975983300072\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  132 Duration =  83.91297194199979\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  133 Duration =  83.91342601999895\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  134 Duration =  83.72450632800064\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  135 Duration =  83.86013109399937\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  136 Duration =  83.7517971289999\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  137 Duration =  83.77324363300067\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  138 Duration =  83.82848800300053\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  139 Duration =  83.72325771700162\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  140 Duration =  83.76646097500088\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  141 Duration =  83.72274215300058\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  142 Duration =  83.73754805799945\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  143 Duration =  83.77821136900093\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  144 Duration =  83.829522136999\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  145 Duration =  84.0651764059985\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  146 Duration =  84.09697602800043\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  147 Duration =  84.02394481199917\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  148 Duration =  83.95893266600069\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  149 Duration =  84.13006465199942\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  150 Duration =  84.02376110099976\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  151 Duration =  84.01223545299945\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  152 Duration =  83.99608270800127\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  153 Duration =  84.0224577099998\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  154 Duration =  83.98178270599965\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  155 Duration =  83.99482935699962\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  156 Duration =  83.96387291999963\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  157 Duration =  83.94972942999993\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  158 Duration =  84.0488396310011\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  159 Duration =  83.98531251700115\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  160 Duration =  83.96819186799985\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  161 Duration =  83.8344289050001\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  162 Duration =  83.84681377299967\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  163 Duration =  84.12918396399982\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  164 Duration =  84.32722505900165\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  165 Duration =  83.92960376700103\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  166 Duration =  83.91086735599856\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  167 Duration =  83.84393433900004\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  168 Duration =  83.98284147199956\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  169 Duration =  83.82359350800107\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  170 Duration =  83.90446763600085\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  171 Duration =  84.23426088700035\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  172 Duration =  85.05964990800021\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  173 Duration =  84.31170195599952\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  174 Duration =  83.92781059499976\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  175 Duration =  83.94884041700061\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  176 Duration =  83.96515632099909\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  177 Duration =  84.0159000019994\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  178 Duration =  83.92259740600093\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  179 Duration =  84.06051822200061\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  180 Duration =  83.98137309399863\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  181 Duration =  83.9662931189996\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  182 Duration =  83.99586499100042\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  183 Duration =  83.944001191001\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  184 Duration =  83.94636283599903\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  185 Duration =  83.95467888500025\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  186 Duration =  84.04222323999966\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  187 Duration =  83.94136825100031\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  188 Duration =  83.98950359400078\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  189 Duration =  84.00097645099959\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  190 Duration =  84.0599667710012\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  191 Duration =  84.01357687600103\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  192 Duration =  84.01450132600075\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  193 Duration =  84.00808864099963\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  194 Duration =  84.00473245800094\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  195 Duration =  83.95224964199952\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  196 Duration =  83.90137839900126\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  197 Duration =  84.01101727300193\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  198 Duration =  83.95637792799971\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  199 Duration =  83.92173884899967\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  200 Duration =  83.9493299809983\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  201 Duration =  83.94357597199996\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  202 Duration =  83.86325424900133\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  203 Duration =  83.88948084199728\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  204 Duration =  84.2061226750011\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  205 Duration =  84.28684895200058\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  206 Duration =  83.8430670580019\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  207 Duration =  83.8911865279988\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  208 Duration =  83.91546957200262\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  209 Duration =  84.0570573820005\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  210 Duration =  83.93203144899962\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  211 Duration =  83.95114941599968\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  212 Duration =  83.89650082099979\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  213 Duration =  83.8393124619979\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  214 Duration =  83.9595710149988\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  215 Duration =  83.94261617299708\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  216 Duration =  83.95255654400171\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  217 Duration =  83.93141108000054\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  218 Duration =  83.92759867999848\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  219 Duration =  83.96115578899844\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  220 Duration =  83.92089032000149\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  221 Duration =  83.97565462199782\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  222 Duration =  84.42709433599885\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  223 Duration =  84.22647635799876\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  224 Duration =  83.98349652899924\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  225 Duration =  83.89790372099742\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  226 Duration =  83.87582035499872\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  227 Duration =  84.88841641999898\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  228 Duration =  84.52516538599957\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  229 Duration =  84.03676223299772\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  230 Duration =  84.03492191799887\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  231 Duration =  84.03710015400065\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  232 Duration =  84.0181828349996\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  233 Duration =  83.95940509000138\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  234 Duration =  83.92045207699994\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  235 Duration =  84.82428887900096\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  236 Duration =  84.41584878600042\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  237 Duration =  84.42619310200098\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  238 Duration =  84.17378830999951\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  239 Duration =  83.95960689000276\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  240 Duration =  83.93993485800092\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  241 Duration =  84.12224199000048\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  242 Duration =  84.10920158599765\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  243 Duration =  84.14586413399957\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  244 Duration =  84.08604753899999\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  245 Duration =  84.0897474150006\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  246 Duration =  84.02462395999828\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  247 Duration =  84.09550826900158\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  248 Duration =  84.11511858599988\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  249 Duration =  84.02835340000092\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      " কে ##ান ##ও গর ##ে ##ফত ##ার হবে না । \n",
      "Epoch  250 Duration =  84.09631462099787\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 250\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "#     gc.collect()\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}')\n",
    "    print(translate(transformer, \"There will be no interruption.\"))\n",
    "    print(\"Epoch \",epoch ,\"Duration = \",end_time-start_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "417bb78d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T13:27:05.303555Z",
     "iopub.status.busy": "2024-03-03T13:27:05.302729Z",
     "iopub.status.idle": "2024-03-03T13:27:05.724260Z",
     "shell.execute_reply": "2024-03-03T13:27:05.723286Z"
    },
    "papermill": {
     "duration": 0.457633,
     "end_time": "2024-03-03T13:27:05.726648",
     "exception": false,
     "start_time": "2024-03-03T13:27:05.269015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('filename.pkl', 'wb') as f:\n",
    "    pickle.dump(transformer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c1ebfe1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T13:27:05.792787Z",
     "iopub.status.busy": "2024-03-03T13:27:05.792451Z",
     "iopub.status.idle": "2024-03-03T13:27:05.830074Z",
     "shell.execute_reply": "2024-03-03T13:27:05.829099Z"
    },
    "papermill": {
     "duration": 0.072479,
     "end_time": "2024-03-03T13:27:05.831983",
     "exception": false,
     "start_time": "2024-03-03T13:27:05.759504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " খাবারের ##ও বা ##ডাব ##াড ##ি ।\n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer,\"food\"))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1272055,
     "sourceId": 2119948,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 165163146,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21421.17727,
   "end_time": "2024-03-03T13:27:08.898940",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-03T07:30:07.721670",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "04a7b272d1ec4d3a869ed0c4913ccbff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "21f0bec826074bbb9bd0c56efafd1684": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "259ef73bc8cb4325bf11e972748c42e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "27c13841e70f41dea33ff86b57b554fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6e01300bf0f84928b10a8668584853c1",
        "IPY_MODEL_92139e5ed9ee406985fc374b020b20cd",
        "IPY_MODEL_fff7cd086c1b4cea885123f4909f3ba6"
       ],
       "layout": "IPY_MODEL_21f0bec826074bbb9bd0c56efafd1684"
      }
     },
     "33d0ad16e2e846eca08f1074cec62d06": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a0cf03e4cb84585b8e5f2b89cec833d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3b8444fab51f4f5bae7d56701fa6ae3e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_92c40bb3f4ba44d2bc557a18b20fc41e",
       "placeholder": "​",
       "style": "IPY_MODEL_259ef73bc8cb4325bf11e972748c42e9",
       "value": " 2.24M/2.24M [00:00&lt;00:00, 9.86MB/s]"
      }
     },
     "5f62e3573e7d46c29d44b632c5df0162": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c10ac932060f4cc4bd52c60de933ed75",
       "max": 2237676.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ee7b05b718314dcf9c0afa8c2eaf0fb2",
       "value": 2237676.0
      }
     },
     "5fb35f9131064ab296c8f0706c1c201e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6827ef5803e34913b9373e2397a0cb57": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6d1d4ff5c96649ddb77fdc0757842dc8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6e01300bf0f84928b10a8668584853c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d74017f1e52c41fe8016d248efa46dea",
       "placeholder": "​",
       "style": "IPY_MODEL_5fb35f9131064ab296c8f0706c1c201e",
       "value": "config.json: 100%"
      }
     },
     "75dd401a788e4bc7a46d98d00577f47d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dfb93bd5ba054e5da505d8ed0261271e",
       "placeholder": "​",
       "style": "IPY_MODEL_6827ef5803e34913b9373e2397a0cb57",
       "value": "vocab.txt: 100%"
      }
     },
     "92139e5ed9ee406985fc374b020b20cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_04a7b272d1ec4d3a869ed0c4913ccbff",
       "max": 491.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cf395a7221ac4f8882b8223d3423fa0f",
       "value": 491.0
      }
     },
     "92c40bb3f4ba44d2bc557a18b20fc41e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c10ac932060f4cc4bd52c60de933ed75": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cf395a7221ac4f8882b8223d3423fa0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d1e666b4c2654f95a45f766bb3d508fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_75dd401a788e4bc7a46d98d00577f47d",
        "IPY_MODEL_5f62e3573e7d46c29d44b632c5df0162",
        "IPY_MODEL_3b8444fab51f4f5bae7d56701fa6ae3e"
       ],
       "layout": "IPY_MODEL_6d1d4ff5c96649ddb77fdc0757842dc8"
      }
     },
     "d74017f1e52c41fe8016d248efa46dea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dfb93bd5ba054e5da505d8ed0261271e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ee7b05b718314dcf9c0afa8c2eaf0fb2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fff7cd086c1b4cea885123f4909f3ba6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_33d0ad16e2e846eca08f1074cec62d06",
       "placeholder": "​",
       "style": "IPY_MODEL_3a0cf03e4cb84585b8e5f2b89cec833d",
       "value": " 491/491 [00:00&lt;00:00, 38.4kB/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
